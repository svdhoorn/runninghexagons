{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2746d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description\n",
    "\n",
    "# TODO\n",
    "# Add a check if the gpx-file in new folder already exists in processed folder. So files are not processed multiple times\n",
    "\n",
    "# Requirements\n",
    "import os\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "\n",
    "# Variables\n",
    "new_gpx_foldername = 'new_gpxfiles'\n",
    "new_gpx_directory = pathlib.Path('.').absolute() / new_gpx_foldername \n",
    "\n",
    "processed_gpx_foldername = 'processed_gpxfiles'\n",
    "processed_gpx_directory = pathlib.Path('.').absolute() / processed_gpx_foldername \n",
    "\n",
    "hexagonfilename = 'hexagonNetherlands.json'\n",
    "hexagonfile = pathlib.Path('.').absolute() / hexagonfilename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55929325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporary gpd to store information per run\n",
    "temp_gdf_hexagons_per_run = []\n",
    "\n",
    "# Proces per gpx-file\n",
    "for filename in os.listdir(new_gpx_directory):\n",
    "    if filename.endswith('.gpx'):\n",
    "        gpxfile = os.path.join(new_gpx_directory, filename)\n",
    "        \n",
    "        gpx = gpd.read_file(gpxfile, layer='tracks')\n",
    "\n",
    "        gdf_gpxline = gpx[gpx['name'].str.contains(\"Running\")]\n",
    "        \n",
    "        date = filename[:10]\n",
    "        gdf_gpxline['date'] = date\n",
    "        \n",
    "        # Replace 'path/to/your/file.geojson' with the actual file path\n",
    "        #file_path = 'C:/Projects/RunningHexagons/data/hexagonNetherlands.geojson'\n",
    "        polygon = gpd.read_file(hexagonfile)\n",
    "\n",
    "        # Perform the intersection\n",
    "        gdf_join = gpd.sjoin(left_df=polygon, right_df=gdf_gpxline,  how=\"inner\", predicate=\"intersects\")\n",
    "        #print(gdf_join)\n",
    "        \n",
    "        #Append data to temp gdf\n",
    "        temp_gdf_hexagons_per_run.append(gdf_join)\n",
    "\n",
    "# Create gdf with information from all gpx-files\n",
    "gdf_new = gpd.GeoDataFrame( pd.concat(temp_gdf_hexagons_per_run, ignore_index=True) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e427bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate information per hexagons (number of runs, first date and last date)\n",
    "df_new_count = gdf_new.groupby('uuid', as_index=False).agg(count=('uuid', 'count'))\n",
    "df_new_first_date = gdf_new.groupby('uuid', as_index=False).agg(first_date=('date', 'min'))\n",
    "df_new_last_date = gdf_new.groupby('uuid', as_index=False).agg(last_date=('date', 'max'))\n",
    "\n",
    "# Merge information per hexagon in one df\n",
    "df_new_all = pd.merge(df_new_count, df_new_first_date, on=[\"uuid\"])\n",
    "df_new_all = pd.merge(df_new_all, df_new_last_date, on=[\"uuid\"])\n",
    "\n",
    "# Merge geometry (gdf) with attribute information (df)\n",
    "gdf_new_temp = gdf_new.merge(df_new_all, on=[\"uuid\"])\n",
    "\n",
    "gdf_new_complete = gdf_new_temp.drop_duplicates(subset=['uuid'])\n",
    "\n",
    "# Select columsn\n",
    "col_list = ['uuid', 'count', 'first_date', 'last_date', 'geometry']\n",
    "gdf_new_complete = gdf_new_complete[col_list]\n",
    "#gdf_new_complete.to_file(\"gdf_new_complete.geojson\", driver='GeoJSON')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8426d861",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "### Merge dataframe of new gpx files with existing hexagons.geojson\n",
    "# Proces is split into two parts, geometry and attributes, which are merged later\n",
    "\n",
    "# Import existing Running Hexagons file\n",
    "rh_filename = 'runninghexagons.geojson'\n",
    "rh_file = pathlib.Path('.').absolute() / rh_filename\n",
    "gdf_existing = gpd.read_file(rh_file)\n",
    "\n",
    "\n",
    "### GEOMETRY\n",
    "col_list_merge = ['uuid', 'geometry']\n",
    "\n",
    "gdf_existing_geometry_merge = gdf_existing[col_list_merge]\n",
    "gdf_new_geometry_merge = gdf_new_complete[col_list_merge]\n",
    "\n",
    "gdf_geometry_merged = pd.concat([gdf_existing_geometry_merge, gdf_new_geometry_merge])\n",
    "\n",
    "# Drop duplicates\n",
    "gdf_geometry_merged = gdf_geometry_merged.drop_duplicates(subset=['uuid'])\n",
    "#gdf_geometry_merged.to_file(\"gdf_geometry_merged.geojson\", driver='GeoJSON')\n",
    "\n",
    "### END GEOMETRY\n",
    "\n",
    "### ATTRIBUTES\n",
    "\n",
    "## Calculate attributes count, first_date and last_date from new gpx-files\n",
    "\n",
    "# Existing\n",
    "df_existing_attr = pd.DataFrame(gdf_existing)\n",
    "col_list_attr = ['uuid', 'count', 'first_date', 'last_date']\n",
    "df_existing_attr = df_existing_attr[col_list_attr]\n",
    "#df_existing_attr.to_csv(\"df_existing_attr.csv\", sep=';', header=True)\n",
    "\n",
    "# New\n",
    "df_new_attr = pd.DataFrame(gdf_new_complete)\n",
    "df_new_attr = df_new_attr[col_list_attr]\n",
    "#df_new_attr.to_csv(\"df_new_attr.csv\", sep=';', header=True)\n",
    "\n",
    "#Concatanete existing en new information\n",
    "df_attr_concat = pd.concat([df_existing_attr, df_new_attr])\n",
    "#df_attr_concat.to_csv(\"df_attr_concat.csv\", sep=';', header=True)\n",
    "\n",
    "# SUM - count\n",
    "df_all_count = df_attr_concat.groupby(['uuid'])['count'].agg('sum')\n",
    "\n",
    "# MIN - first_date\n",
    "df_all_first_date = df_attr_concat.groupby('uuid', as_index=False).agg(first_date=('first_date', 'min'))\n",
    "\n",
    "# MAX - last_date\n",
    "df_all_last_date = df_attr_concat.groupby('uuid', as_index=False).agg(last_date=('last_date', 'max'))\n",
    "\n",
    "# Merge information\n",
    "df_all_attr_merged = pd.merge(df_all_count, df_all_first_date, how = 'inner', on=[\"uuid\"])\n",
    "df_all_attr_merged = pd.merge(df_all_attr_merged, df_all_last_date, on=[\"uuid\"])\n",
    "#df_all_attr_merged.to_csv(\"df_rh_all_attr_merged.csv\", sep=';', header=True)\n",
    "\n",
    "### END ATTRIBUTES\n",
    "\n",
    "### MERGE GEOMETRY AND ATTRIBUTES\n",
    "# Merge geometry (gdf) with attribute information (df)\n",
    "gdf_all_runs = gdf_geometry_merged.merge(df_all_attr_merged, on=[\"uuid\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e8acab",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write geojsonfile\n",
    "if(os.path.isfile(\"runninghexagons.geojson\")):\n",
    "    os.remove(\"runninghexagons.geojson\")\n",
    "    print(\"File Deleted successfully\")\n",
    "else:\n",
    "    print(\"File does not exist\")\n",
    "\n",
    "\n",
    "gdf_all_runs.to_file(\"runninghexagons.geojson\", driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9198f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move data from new_gpxfiles dir to processed_gpxfiles dir\n",
    "# Move files before writing the geojson, if there are files already in the processed folder the script stops, so no files are not processed multiple times\n",
    "# Gather all files in source folder\n",
    "allfiles = os.listdir(new_gpx_directory)\n",
    " \n",
    "# Iterate on all files to move them to destination folder\n",
    "for f in allfiles:\n",
    "    src_path = os.path.join(new_gpx_directory, f)\n",
    "    dst_path = os.path.join(processed_gpx_directory, f)\n",
    "    os.rename(src_path, dst_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
